# `EdgeBatchEnv` 环境形式化描述

## 1. 环境概述

`EdgeBatchEnv` 是一个为多智能体强化学习（MARL）设计的、事件驱动的模拟环境。它旨在模拟一个包含 $N$ 个用户和 $M$ 个边缘服务器的计算系统。环境的核心目标是通过智能体的协同决策，在一个连续的时间维度上，共同优化系统的数据新鲜度（以信息年龄 AoI 衡量）和用户效用。

## 2. 核心实体

*   **用户 (User)**: 系统中有 $N$ 个用户，索引为 $i \in \{0, 1, ..., N-1\}$。每个用户独立地、循环地经历“生成任务 -> 决策等待时长 -> 决策分配服务器 -> 等待任务完成”的生命周期。
*   **服务器 (Server)**: 系统中有 $M$ 个服务器，索引为 $j \in \{0, 1, ..., M-1\}$。每个服务器拥有一个先进先出（FIFO）的任务队列，队列长度上限为 $Q_{max}$。服务器可以决策是否以及何时以批处理（Batch Processing）的方式处理队列中的任务。

## 3. 事件驱动机制

与传统的以固定时间步长（timestep）推进的环境不同，本环境由一个全局事件队列驱动。系统状态的改变仅在离散的事件点发生。在任意时刻 $t$，环境会从所有用户和服务器的待处理事件中，选择时间戳最早的那个来执行。这种机制能够精确模拟连续时间下的异步行为。

事件主要分为以下几类：
*   **用户事件**: 用户需要做出决策（等待多长时间，或分配到哪个服务器）。
*   **服务器事件**: 服务器需要做出决策（是否开始处理一个批次），或一个批次处理完成并需要进行奖励分配。

## 4. 形式化定义 (MDP)

该环境可以被建模为一个多智能体马尔可夫决策过程 (Multi-Agent Markov Decision Process, MARL-MDP)。

### 4.1. 状态空间 (State Space)

*   **用户 $i$ 的内部状态 $S_i^u$**:
    $$
    S_i^u = \{t_i^{k-1}, q_i^{k-1}, w_i, \text{phase}_i\}
    $$
    *   $t_i^{k-1}$: 用户 $i$ 上一个（第 k-1 个）任务的生成时间。
    *   $q_i^{k-1}$: 用户 $i$ 上一个任务的完成时间。
    *   $w_i$: 用户 $i$ 当前决策的等待时间。
    *   $\text{phase}_i$: 用户 $i$ 的当前阶段, $\text{phase}_i \in \{\text{'wait'}, \text{'assign'}\}$。

*   **服务器 $j$ 的内部状态 $S_j^s$**:
    $$
    S_j^s = \{Q_j\}
    $$
    *   $Q_j$: 服务器 $j$ 的任务队列, $Q_j = [(u_1, t_1), (u_2, t_2), ...]$，其中 $(u, t)$ 代表来自用户 $u$ 的任务在 $t$ 时刻到达。队列长度 $|Q_j| \le Q_{max}$。

*   **全局状态 $S_g$ (供 Critic 网络使用)**:
    $$
    S_g = (S_0^u, ..., S_{N-1}^u, |Q_0|, ..., |Q_{M-1}|, T_0, ..., T_{M-1}, F_0, ..., F_{M-1})
    $$
    *   $S_i^u$: 所有用户的部分状态 $(t_i^{k-1}, q_i^{k-1}, w_i)$。
    *   $|Q_j|$: 所有服务器的队列长度。
    *   $T_j$: 所有服务器队列中任务的到达时间序列（补齐至 $Q_{max}$）。
    *   $F_j$: 所有服务器的忙闲状态 $F_j \in \{0, 1\}$ (1=空闲, 0=忙碌)。

*   **用户 $i$ 的观察 $O_i^u$ (供 Actor 网络使用)**:
    $$
    O_i^u = (t_i^{k-1}, q_i^{k-1}, w_i, |Q_0|, ..., |Q_{M-1}|, F_0, ..., F_{M-1})
    $$
    *   它包含用户自身的部分状态，以及所有服务器的队列长度和忙闲状态，构成了用户的局部视野。

*   **服务器 $j$ 的观察 $O_j^s$ (供 Actor 网络使用)**:
    $$
    O_j^s = (|Q_j|, T_j)
    $$
    *   它包含服务器自身的队列长度和队列中任务的到达时间序列。

### 4.2. 动作空间 (Action Space)

*   **用户 $i$ 的动作 $A_i^u$**:
    *   当 $\text{phase}_i = \text{'wait'}$: $A_i^u = w_i$，其中 $w_i$ 是一个从 $[1, W_{max}]$ 中选择的连续值，代表等待时长。
    *   当 $\text{phase}_i = \text{'assign'}$: $A_i^u = j$，其中 $j \in \{0, ..., M-1\}$，代表选择的服务器索引。

*   **服务器 $j$ 的动作 $A_j^s$**:
    $A_j^s \in \{0, 1\}$
    *   $A_j^s = 0$: 不处理任务。
    *   $A_j^s = 1$: 开始处理一个批次。批次大小 $B_j = \min(|Q_j|, B_{max})$。

### 4.3. 状态转移函数 $P(S' | S, A)$

状态转移是确定性的，由环境中的 `step_*` 系列函数定义：
1.  **用户等待**: `step_user_wait(i, w_i)` $\rightarrow$ 用户 $i$ 的 $\text{phase}_i$ 变为 $\text{'assign'}$，其下一个事件时间被设置为 $t + w_i$。
2.  **用户分配**: `step_user_assign(i, j)` $\rightarrow$ 用户 $i$ 的任务被添加到服务器 $j$ 的队列 $Q_j$ 中。用户 $i$ 进入休眠状态，等待其任务被处理完成。
3.  **服务器开始处理**: `step_server_start(j, a_j)` $\rightarrow$ 若 $a_j=1$，服务器 $j$ 的下一个事件被设置为 $\text{'end'}$，其时间戳为 $t + P(B_j)$，其中 $P(B_j)$ 是处理大小为 $B_j$ 的批次所需的时间。
4.  **服务器结束处理**: `step_server_end(j)` $\rightarrow$ 服务器 $j$ 从队列 $Q_j$ 中移除 $B_j$ 个任务。对于每个完成的任务，其所属的用户被唤醒（即为其创建新的 'wait' 事件），并计算相应的奖励。

### 4.4. 奖励函数 $R(S, A)$

奖励仅在 `step_server_end` 事件中产生。

*   **用户奖励 $R_i^u$**:
    当用户 $i$ 的第 $k$ 个任务在 $q_i^k$ 时刻完成时，其奖励与信息年龄（AoI）的积分面积直接相关。奖励函数旨在最小化这个面积以及相关的等待成本。
    $R_i^u = - \left[ 0.5 \times ((q_i^k - t_i^{k-1})^2 - (q_i^k - t_i^k)^2) - \lambda \times (q_i^{k-1} - t_i^{k-1} + w_i) \right]$
    *   $0.5 \times (...)$ 项是 AoI 梯形面积的精确计算。
    *   $\lambda \times (...)$ 项是与任务总生命周期（包含等待时间）相关的线性惩罚项。
    *   $\lambda$ 是一个关键的权衡超参数。
    *   整个表达式取负，因为强化学习的目标是最大化奖励，而我们的目标是最小化 AoI 和成本。

*   **服务器奖励 $R_j^s$**:
    服务器 $j$ 在完成一个批次处理后获得的奖励，被定义为该批次中所有任务的用户奖励之和。
    $R_j^s = \sum_{i \in \text{Batch}} R_i^u$